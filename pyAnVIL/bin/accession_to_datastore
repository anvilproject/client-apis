#!/usr/bin/env python3
"""Map workspace to study_accession ~ data_store."""

from collections import defaultdict
import logging
import click
from os.path import isdir
import json
from subprocess import Popen, PIPE

from anvil.terra.reconciler import Entities

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(filename)s %(levelname)-8s %(message)s')

@click.group()
@click.pass_context
def cli(ctx):
    """Set up context, main entrypoint."""
    # ensure that ctx.obj exists and is a dict 
    # in case we want to eventually chain these commands together into a pipeline
    ctx.ensure_object(dict)


# @click.command()
@cli.command('all')
@click.option('--output_path', required=True, help='Where to read output from transform.')
@click.option('--accession_mapping', required=True, help='Where to read json file study_accession,etc.')
@click.option('--google_project', required=True, help='What google project will host the FHIR server')
@click.option('--google_location', required=True, help='The google region where the FHIR server will reside')
@click.option('--google_dataset', required=True, help='The google dataset will host all FHIR datastores')
@click.option('--google_bucket', required=True, help='The google bucket that will host the IG and transformed workspaces json')
@click.option('--user_project', required=True, help='The google user_project')

def accession_to_datastore(output_path, accession_mapping, google_project, google_location, google_dataset, google_bucket, user_project):
    """Map terra.workspace_name to fhir.data_store using spreadsheet."""
    # load spreadsheet
    accessions = json.load(open(accession_mapping, "r"))
    # >> dict_keys(['workspace_name', 'study_accession', 'dataUseRestriction'])
    # create hash with lower-case workspace name as key
    accessions = {a['workspace_name'].lower(): a for a in accessions}

    # load workspaces we've transformed
    terra_output_path = f"{output_path}/terra.sqlite"
    entities = Entities(terra_output_path=terra_output_path, user_project=user_project)
    _all_workspaces = entities.get_by_name('workspace')
    workspaces = [{'workspace_name': w.name, 'reconciler_name': w.attributes.reconciler_name} for w in _all_workspaces]
    # >> dict_keys(['workspace_name', 'reconciler_name'])

    # ensure json directory exists
    for w in workspaces:
        path = f"{output_path}/{w['reconciler_name']}/{w['workspace_name']}"
        if not isdir(path):
            logging.getLogger(__name__).warning(f"no directory found, likely transform had issues. see {output_path}/qa-report.md skipping. {path}")
        else:
            w['path'] = path            
    workspaces = {w['workspace_name'].lower(): w for w in workspaces if 'path' in w}
    
    # add spreadsheet fields to workspace
    for workspace_name_lower in workspaces.keys():
        w = workspaces[workspace_name_lower]
        if workspace_name_lower not in accessions:
            logging.getLogger(__name__).warning(f"no accession found, likely spreadsheet out of date. assigning to pending. {w['workspace_name']}")
            w['accession'] = 'registration pending'
            w['dataUseRestriction'] = None
        else:
            w['accession'] = accessions[workspace_name_lower]['study_accession']
            w['dataUseRestriction'] = accessions[workspace_name_lower]['dataUseRestriction']

    # map accession to datastore
    data_stores = defaultdict(list)
    for w in workspaces.values():
        # TODO append -{w['dataUseRestriction']} but data is messy :-(
        data_stores[f"{w['accession']}"].append(w['path'])
    # rename weird enums from spreadsheet
    if '--' in data_stores:
        data_stores['public'].extend(data_stores.pop('--'))
    if 'registration pending' in data_stores:
        data_stores['pending'].extend(data_stores.pop('registration pending'))
    if 'available through EGA' in data_stores:
        data_stores['ega'].extend(data_stores.pop('available through EGA'))

    import pprint 
    pprint.pprint(data_stores)

    # # create data_stores
    # logging.getLogger(__name__).info("Starting to create data_store per accession...")
    # for data_store in data_stores.keys():        
    #     cmd = ["gcloud", "beta", "healthcare", "fhir-stores", "create", data_store, f"--dataset={google_dataset}", f"--location={google_location}", "--version=R4", "--enable-update-create"]
    #     p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #     stdout, stderr = p.communicate()
    #     stdout = stdout.decode("utf-8").rstrip()
    #     stderr = stderr.decode("utf-8").rstrip()
    #     rc = p.returncode
    #     if rc == 0:
    #         logging.getLogger(__name__).info(f"created data_store={data_store}")
    #     else:
    #         if "ALREADY_EXISTS" in stderr:
    #             logging.getLogger(__name__).info(f"FYI {data_store} <{stderr}> Continuing...")
    #         else:
    #             logging.getLogger(__name__).warning(stdout)
    #             logging.getLogger(__name__).warning(stderr)
    #             raise Exception(f"{cmd} returned {rc}")
    # logging.getLogger(__name__).info("Created all data_stores")

    # # load data_stores with IG
    # logging.getLogger(__name__).info("Starting to load ImplementationGuide to each data_store...")
    # for data_store in data_stores.keys():        
    #     cmd = ["gcloud", "beta", "healthcare", "fhir-stores", "import",  "gcs", data_store, f"--dataset={google_dataset}", f"--location={google_location}", f"--gcs-uri=gs://{google_bucket}/IG/*.json", "--content-structure=resource-pretty"]
    #     p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #     stdout, stderr = p.communicate()
    #     stdout = stdout.decode("utf-8").rstrip()
    #     stderr = stderr.decode("utf-8").rstrip()
    #     rc = p.returncode
    #     if rc == 0:
    #         logging.getLogger(__name__).info(f"Loaded IG into data_store={data_store}")
    #     else:
    #         logging.getLogger(__name__).warning(stdout)
    #         logging.getLogger(__name__).warning(stderr)
    #         raise Exception(f"{cmd} returned {rc}")
    # logging.getLogger(__name__).info("Completed loaded IG into all data_stores")

    # logging.getLogger(__name__).info("Enabling ImplementationGuide in each data_store...")
    # for data_store in data_stores.keys():        
    #     cmd = ["enable_implementation_guide.sh", data_store]
    #     p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #     stdout, stderr = p.communicate()
    #     stdout = stdout.decode("utf-8").rstrip()
    #     stderr = stderr.decode("utf-8").rstrip()
    #     rc = p.returncode
    #     if rc == 0:
    #         logging.getLogger(__name__).info(f"Enabled IG in data_store={data_store}")
    #     else:
    #         logging.getLogger(__name__).warning(stdout)
    #         logging.getLogger(__name__).warning(stderr)
    #         raise Exception(f"{cmd} returned {rc}")
    # logging.getLogger(__name__).info("Completed enabling IG in all data_stores")

    # # load data into target data_stores
    # logging.getLogger(__name__).info("Loading study & subject data into each data_store...")
    # for data_store, directories in data_stores.items():
    #     for directory in directories:
    #         path = f"{directory}".replace(output_path,"").replace("//","/")    
    #         uri = f"gs://{google_bucket}{path}"
    #         cmd = ["load_project_data.sh", data_store, uri]
    #         p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #         stdout, stderr = p.communicate()
    #         stdout = stdout.decode("utf-8").rstrip()
    #         stderr = stderr.decode("utf-8").rstrip()
    #         rc = p.returncode
    #         if rc == 0:
    #             logging.getLogger(__name__).info(f"Loaded study data into data_store={data_store} from {path}")
    #         else:
    #             logging.getLogger(__name__).warning(f"Failed to study data into data_store={data_store} from {path}")
    #             logging.getLogger(__name__).warning(stdout)
    #             logging.getLogger(__name__).warning(stderr)
    #             raise Exception(f"{cmd} returned {rc}")
    #         cmd = ["load_subject_data.sh", data_store, uri]
    #         p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #         stdout, stderr = p.communicate()
    #         stdout = stdout.decode("utf-8").rstrip()
    #         stderr = stderr.decode("utf-8").rstrip()
    #         rc = p.returncode
    #         if rc == 0:
    #             logging.getLogger(__name__).info(f"Loaded subject data into data_store={data_store} from {path}")
    #         else:
    #             if "resolves to zero GCS objects" in stderr:
    #                 logging.getLogger(__name__).info(f"FYI {data_store} <{stderr}> Continuing...")
    #             else:
    #                 logging.getLogger(__name__).warning(f"Failed to load subject data into data_store={data_store} from {path}")
    #                 logging.getLogger(__name__).warning(stdout)
    #                 logging.getLogger(__name__).warning(stderr)
    #                 raise Exception(f"{cmd} returned {rc}")
    
    # # load study data into public
    # logging.getLogger(__name__).info("Loading controlled study data (only!) into public data_store...")
    # for data_store, directories in data_stores.items():
    #     if data_store == 'public':
    #         # we've already loaded this, so skip
    #         continue
    #     data_store = 'public'
    #     for directory in directories:
    #         path = f"{directory}".replace(output_path,"").replace("//","/")    
    #         uri = f"gs://{google_bucket}{path}"
    #         cmd = ["load_project_data.sh", data_store, uri]
    #         p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #         stdout, stderr = p.communicate()
    #         stdout = stdout.decode("utf-8").rstrip()
    #         stderr = stderr.decode("utf-8").rstrip()
    #         rc = p.returncode
    #         if rc == 0:
    #             logging.getLogger(__name__).info(f"Loaded study data into data_store={data_store} from {path}")
    #         else:
    #             logging.getLogger(__name__).warning(f"Failed to study data into data_store={data_store} from {path}")
    #             logging.getLogger(__name__).warning(stdout)
    #             logging.getLogger(__name__).warning(stderr)
    #             raise Exception(f"{cmd} returned {rc}")

    logging.getLogger(__name__).info("Completed loading study & project in all data_stores")


if __name__ == '__main__':
    try:
        cli(obj={})
    except Exception as e:
        logging.getLogger(__name__).exception(e)
        
