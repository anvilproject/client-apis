#!/usr/bin/env python3
"""Map workspace to study_accession ~ data_store."""

from collections import defaultdict
import logging
import click
from os.path import isdir
import json
from subprocess import Popen, PIPE
import os

from anvil.terra.reconciler import Entities

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(filename)s %(levelname)-8s %(message)s')


@click.group()
@click.pass_context
def cli(ctx):
    """Set up context, main entrypoint."""
    # ensure that ctx.obj exists and is a dict
    # in case we want to eventually chain these commands together into a pipeline
    ctx.ensure_object(dict)


def load_spreadsheet(accession_mapping, output_path, user_project, data_store_filter):
    """Load spreadsheet."""
    accessions = json.load(open(accession_mapping, "r"))
    # >> dict_keys(['workspace_name', 'study_accession', 'dataUseRestriction'])
    # create hash with lower-case workspace name as key
    accessions = {a['workspace_name'].lower(): a for a in accessions}

    # load workspaces we've transformed
    terra_output_path = f"{output_path}/terra.sqlite"
    entities = Entities(terra_output_path=terra_output_path, user_project=user_project)
    _all_workspaces = entities.get_by_name('workspace')
    workspaces = [{'workspace_name': w.name, 'reconciler_name': w.attributes.reconciler_name} for w in _all_workspaces]
    # >> dict_keys(['workspace_name', 'reconciler_name'])

    # ensure json directory exists
    for w in workspaces:
        path = f"{output_path}/{w['reconciler_name']}/{w['workspace_name']}"
        if not isdir(path):
            logging.getLogger(__name__).warning(f"no directory found, likely transform had issues. see {output_path}/qa-report.md skipping. {path}")
        else:
            w['path'] = path
    workspaces = {w['workspace_name'].lower(): w for w in workspaces if 'path' in w}

    # add spreadsheet fields to workspace
    for workspace_name_lower in workspaces.keys():
        w = workspaces[workspace_name_lower]
        if workspace_name_lower not in accessions:
            logging.getLogger(__name__).warning(f"no accession found, likely spreadsheet out of date. assigning to pending. {w['workspace_name']}")
            w['accession'] = 'registration pending'
            w['dataUseRestriction'] = None
        else:
            w['accession'] = accessions[workspace_name_lower]['study_accession']
            w['dataUseRestriction'] = accessions[workspace_name_lower]['dataUseRestriction']

    # map accession to datastore
    data_stores = defaultdict(list)
    for w in workspaces.values():
        accession = w['accession']
        dataUseRestriction = w['dataUseRestriction']
        # rename weird enums from spreadsheet
        if '--' in accession:
            accession = 'public'
            logging.getLogger(__name__).info(f"{w['workspace_name']} in public")
        if 'registration pending' in accession:
            accession = 'pending'
        if 'available through EGA' in accession:
            accession = 'pending'
            logging.getLogger(__name__).info(f"{w['workspace_name']} in pending (EGA is not dbGAP)")
        if accession in ['public', 'pending']:
            dataUseRestriction = ''
        if dataUseRestriction in ['not applicable', None]:
            dataUseRestriction = ''
        if dataUseRestriction:
            dataUseRestriction = f'-{dataUseRestriction}'
        elif not accession == 'public':
            accession = 'pending'
            logging.getLogger(__name__).info(f"{w['workspace_name']} in pending (No dataUseRestriction)")
        data_stores[f"{accession}{dataUseRestriction}"].append(w['path'])
    if data_store_filter:
        to_pop = []
        for k in data_stores:
            if not k == data_store_filter:
                to_pop.append(k)
        for k in to_pop:
            data_stores.pop(k)
    return data_stores


@cli.command('load')
@click.option('--output_path', required=True, help='Where to read output from transform.')
@click.option('--accession_mapping', required=True, help='Where to read json file study_accession,etc.')
@click.option('--google_bucket', required=True, help='The google bucket that will host the IG and transformed workspaces json')
@click.option('--user_project', required=True, help='The google user_project')
@click.option('--data_store_filter', required=False, default=None, help='Only load this data_store')
def load_data(output_path, accession_mapping, google_bucket, user_project, data_store_filter):
    """Load spreadsheet."""
    data_stores = load_spreadsheet(accession_mapping, output_path, user_project, data_store_filter)
    # load data into target data_stores
    logging.getLogger(__name__).info("Loading study & subject data into each data_store...")
    for data_store, directories in data_stores.items():
        for directory in directories:

            # set partOf
            # create linkage b/t public->protected
            public_base_url = f"https://healthcare.googleapis.com/v1beta1/projects/{os.environ['GOOGLE_PROJECT']}/locations/{os.environ['GOOGLE_LOCATION']}/datasets/{os.environ['GOOGLE_DATASET']}/fhirStores/public/fhir"
            protected_base_url = f"https://healthcare.googleapis.com/v1beta1/projects/{os.environ['GOOGLE_PROJECT']}/locations/{os.environ['GOOGLE_LOCATION']}/datasets/{os.environ['GOOGLE_DATASET']}/fhirStores/{data_store}/fhir"
            research_study = json.load(open(f"{directory}/ResearchStudy.json", "r"))
            research_study['partOf'] = [
                {"reference": f"{public_base_url}/ResearchStudy/AnVIL"},
                {"reference": f"{protected_base_url}/ResearchStudy/{research_study['id']}"},
            ]
            json.dump(research_study, open(f"{directory}/ResearchStudy.json", "w"))

            path = f"{directory}".replace(output_path, "").replace("//", "/")
            uri = f"gs://{google_bucket}{path}"
            # copy data to bucket
            cmd = ['gsutil', '-m', 'cp', '-r', f'{directory}', uri]
            p = Popen(cmd, stdout=PIPE, stderr=PIPE)
            stdout, stderr = p.communicate()
            stdout = stdout.decode("utf-8").rstrip()
            stderr = stderr.decode("utf-8").rstrip()
            rc = p.returncode
            if rc == 0:
                logging.getLogger(__name__).info(f"Copied data into bucket={uri} from {directory}")
            else:
                logging.getLogger(__name__).warning(f"Failed to copy data to bucket={uri} from {directory}")
                logging.getLogger(__name__).warning(stdout)
                logging.getLogger(__name__).warning(stderr)
                raise Exception(f"{cmd} returned {rc}")

            cmd = ["load_study_data.sh", data_store, uri]
            p = Popen(cmd, stdout=PIPE, stderr=PIPE)
            stdout, stderr = p.communicate()
            stdout = stdout.decode("utf-8").rstrip()
            stderr = stderr.decode("utf-8").rstrip()
            rc = p.returncode
            if rc == 0:
                logging.getLogger(__name__).info(f"Loaded study data into data_store={data_store} from {path}")
            else:
                logging.getLogger(__name__).warning(f"Failed to study data into data_store={data_store} from {path}")
                logging.getLogger(__name__).warning(stdout)
                logging.getLogger(__name__).warning(stderr)
                raise Exception(f"{cmd} returned {rc}")

            cmd = ["load_subject_data.sh", data_store, uri]
            p = Popen(cmd, stdout=PIPE, stderr=PIPE)
            stdout, stderr = p.communicate()
            stdout = stdout.decode("utf-8").rstrip()
            stderr = stderr.decode("utf-8").rstrip()
            rc = p.returncode
            if rc == 0:
                logging.getLogger(__name__).info(f"Loaded subject data into data_store={data_store} from {path}")
            else:
                if "resolves to zero GCS objects" in stderr:
                    logging.getLogger(__name__).info(f"FYI {data_store} <{stderr}> Continuing...")
                else:
                    logging.getLogger(__name__).warning(f"Failed to load subject data into data_store={data_store} from {path}")
                    logging.getLogger(__name__).warning(stdout)
                    logging.getLogger(__name__).warning(stderr)
                    raise Exception(f"{cmd} returned {rc}")

    # load study data into public
    logging.getLogger(__name__).info("Loading controlled study data (only!) into public data_store...")
    for data_store, directories in data_stores.items():
        if data_store == 'public':
            # we've already loaded this, so skip
            continue
        data_store = 'public'
        for directory in directories:
            path = f"{directory}".replace(output_path, "").replace("//", "/")
            uri = f"gs://{google_bucket}{path}"
            cmd = ["load_project_data.sh", data_store, uri]
            p = Popen(cmd, stdout=PIPE, stderr=PIPE)
            stdout, stderr = p.communicate()
            stdout = stdout.decode("utf-8").rstrip()
            stderr = stderr.decode("utf-8").rstrip()
            rc = p.returncode
            if rc == 0:
                logging.getLogger(__name__).info(f"Loaded study data into data_store={data_store} from {path}")
            else:
                logging.getLogger(__name__).warning(f"Failed to study data into data_store={data_store} from {path}")
                logging.getLogger(__name__).warning(stdout)
                logging.getLogger(__name__).warning(stderr)
                raise Exception(f"{cmd} returned {rc}")


@cli.command('initialize')
@click.option('--output_path', required=True, help='Where to read output from transform.')
@click.option('--accession_mapping', required=True, help='Where to read json file study_accession,etc.')
@click.option('--google_location', required=True, help='The google region where the FHIR server will reside')
@click.option('--google_dataset', required=True, help='The google dataset will host all FHIR datastores')
@click.option('--google_bucket', required=True, help='The google bucket that will host the IG and transformed workspaces json')
@click.option('--user_project', required=True, help='The google user_project')
@click.option('--data_store_filter', required=False, default=None, help='Only load this data_store')
def create_data_stores(output_path, accession_mapping, google_location, google_dataset, google_bucket, user_project, data_store_filter):
    """Map terra.workspace_name to fhir.data_store using spreadsheet."""
    # load spreadsheet
    data_stores = load_spreadsheet(accession_mapping, output_path, user_project, data_store_filter)

    # create data_stores
    logging.getLogger(__name__).info("Starting to create data_store per accession...")
    for data_store in data_stores.keys():
        cmd = ["gcloud", "beta", "healthcare", "fhir-stores", "create", data_store, f"--dataset={google_dataset}", f"--location={google_location}", "--version=R4", "--enable-update-create"]
        p = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
        stdout = stdout.decode("utf-8").rstrip()
        stderr = stderr.decode("utf-8").rstrip()
        rc = p.returncode
        if rc == 0:
            logging.getLogger(__name__).info(f"created data_store={data_store}")
        else:
            if "ALREADY_EXISTS" in stderr:
                logging.getLogger(__name__).info(f"FYI {data_store} <{stderr}> Continuing...")
            else:
                logging.getLogger(__name__).warning(stdout)
                logging.getLogger(__name__).warning(stderr)
                raise Exception(f"{cmd} returned {rc}")
    logging.getLogger(__name__).info("Created all data_stores")

    # load data_stores with IG
    logging.getLogger(__name__).info("Starting to load ImplementationGuide to each data_store...")
    for data_store in data_stores.keys():
        cmd = ["gcloud", "beta", "healthcare", "fhir-stores", "import", "gcs", data_store, f"--dataset={google_dataset}", f"--location={google_location}", f"--gcs-uri=gs://{google_bucket}/IG/*.json", "--content-structure=resource-pretty"]
        p = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
        stdout = stdout.decode("utf-8").rstrip()
        stderr = stderr.decode("utf-8").rstrip()
        rc = p.returncode
        if rc == 0:
            logging.getLogger(__name__).info(f"Loaded IG into data_store={data_store}")
        else:
            logging.getLogger(__name__).warning(stdout)
            logging.getLogger(__name__).warning(stderr)
            raise Exception(f"{cmd} returned {rc}")
    logging.getLogger(__name__).info("Completed loaded IG into all data_stores")

    logging.getLogger(__name__).info("Enabling ImplementationGuide in each data_store...")
    for data_store in data_stores.keys():
        cmd = ["enable_implementation_guide.sh", data_store]
        p = Popen(cmd, stdout=PIPE, stderr=PIPE)
        stdout, stderr = p.communicate()
        stdout = stdout.decode("utf-8").rstrip()
        stderr = stderr.decode("utf-8").rstrip()
        rc = p.returncode
        if rc == 0:
            logging.getLogger(__name__).info(f"Enabled IG in data_store={data_store}")
        else:
            logging.getLogger(__name__).warning(stdout)
            logging.getLogger(__name__).warning(stderr)
            raise Exception(f"{cmd} returned {rc}")
    logging.getLogger(__name__).info("Completed enabling IG in all data_stores")

    # # tag data store with label
    # logging.getLogger(__name__).info("Tagging all data stores")
    # for data_store, directories in data_stores.items():
    #     paths = {"labels": {
    #         f"{d}".replace(output_path,"").replace("//","/").lower().split("/")[-1]: "-" for d in directories
    #     }}
    #     paths = json.dumps(paths, separators=(',', ':'))
    #     cmd = ["add_label_to_datastore.sh", data_store, paths]
    #     p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #     stdout, stderr = p.communicate()
    #     stdout = stdout.decode("utf-8").rstrip()
    #     stderr = stderr.decode("utf-8").rstrip()
    #     rc = p.returncode

    #     if rc == 0 and "error" not in stdout:
    #         logging.getLogger(__name__).info(f"Tagged data_store={data_store}")
    #     else:
    #         logging.getLogger(__name__).warning(f"Failed tag data_store={data_store}")
    #         logging.getLogger(__name__).warning(stdout)
    #         logging.getLogger(__name__).warning(stderr)
    #         raise Exception(f"{cmd} returned {rc}")
    #     if data_store == "public":
    #         continue
    #     data_store = "public"
    #     cmd = ["add_label_to_datastore.sh", data_store, paths]
    #     p = Popen(cmd, stdout=PIPE, stderr=PIPE)
    #     stdout, stderr = p.communicate()
    #     stdout = stdout.decode("utf-8").rstrip()
    #     stderr = stderr.decode("utf-8").rstrip()
    #     rc = p.returncode
    #     if rc == 0:
    #         logging.getLogger(__name__).info(f"Tagged data_store={data_store}")
    #     else:
    #         logging.getLogger(__name__).warning(f"Failed to tag data_store={data_store}")
    #         logging.getLogger(__name__).warning(stdout)
    #         logging.getLogger(__name__).warning(stderr)
    #         raise Exception(f"{cmd} returned {rc}")

    logging.getLogger(__name__).info("Completed loading study & project in all data_stores")


if __name__ == '__main__':
    try:
        cli(obj={})
    except Exception as e:
        logging.getLogger(__name__).exception(e)
